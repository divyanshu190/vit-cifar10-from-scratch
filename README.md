
# Project Title

Vision Transformer (ViT-Small) implemented completely from scratch in PyTorch (no pretrained weights).  
Includes patch embedding, multi-head self-attention, MLP blocks, CLS token, cosine LR schedule, and strong data augmentation.  
Trained on CIFAR-10 to achieve ~90% test accuracy after 200 epochs.

## Features
- Implemented from scratch in PyTorch
- Patch Embedding, Multi-Head Self-Attention, MLP blocks, CLS token
- Cosine LR scheduler, strong data augmentation
- ~90% CIFAR-10 test accuracy
## Project Structure
vit.py — model definition
train.py — training loop
requirements.txt — dependencies
## Installation

Install my-project with npm

```bash
  pip install -r requirements.txt
```
    
## Usage/Examples

```javascript
python train.py
```


## Results
~90% test accuracy after 200 epochs on CIFAR-10
## License

[MIT](https://choosealicense.com/licenses/mit/)
