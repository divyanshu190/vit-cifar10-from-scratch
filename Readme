# Vision Transformer (ViT-Small) on CIFAR-10 — Training from Scratch

This repository demonstrates how to train a Vision Transformer (ViT-Small) from scratch on the CIFAR-10 dataset using PyTorch and timm, without any ImageNet pretraining.

The project shows that with a strong training strategy—specifically heavy data augmentation, Mixup/CutMix, Exponential Moving Average (EMA), and cosine learning rate scheduling—transformer-based vision models can achieve competitive performance on small datasets such as CIFAR-10.

---

## Project Overview

Vision Transformers are typically trained on large-scale datasets before fine-tuning. In this work, the model is trained entirely from scratch on CIFAR-10 by adapting the architecture and applying modern regularization techniques.

Key ideas explored:
- Adapting ViT patch size for small-resolution images
- Using strong augmentation to compensate for lack of convolutional inductive bias
- Stabilizing training with EMA
- Improving generalization with Mixup and CutMix

---

## Model Architecture

- Model: Vision Transformer (ViT-Small)
- Image Size: 32 x 32
- Patch Size: 4 x 4
- Number of Patches: 8 x 8 = 64
- Embedding Dimension: 384
- Attention Heads: 12
- Classifier: CLS token with linear head
- Number of Classes: 10 (CIFAR-10)

Reducing the patch size is critical for CIFAR-10. Using the default 16 x 16 patches would produce too few tokens and limit the effectiveness of self-attention.

---

## Dataset

CIFAR-10:
- 60,000 RGB images
- 10 classes
- Resolution: 32 x 32

The dataset is automatically downloaded using torchvision.

---

## Training Strategy

### Data Augmentation
- Random Crop with padding
- Random Horizontal Flip
- RandAugment

Strong augmentation is essential for Vision Transformers due to the absence of convolutional inductive bias.

---

### Mixup and CutMix
- Enabled throughout training
- Produces soft labels
- Improves robustness and generalization

Loss Function:
- SoftTargetCrossEntropy

---

### Exponential Moving Average (EMA)
- Maintains a moving average of model weights
- EMA weights are used for evaluation
- Improves training stability and final accuracy

---

### Optimization

- Optimizer: AdamW
- Learning Rate: 3e-4
- Weight Decay: 0.05
- Scheduler: Cosine Annealing
- Epochs: 200

This setup follows standard best practices for training transformer-based vision models.

---

## Project Structure

```

.
├── train.py          # Training loop and evaluation
├── requirements.txt  # Python dependencies
├── data/             # CIFAR-10 dataset (auto-downloaded)
└── README.md         # Project documentation

```

---

## Installation

Install the required dependencies:

```

pip install torch torchvision timm

```

---

## Usage

To train the model from scratch, run:

```

python train.py

```

The script will:
- Download CIFAR-10 automatically
- Train the Vision Transformer for 200 epochs
- Evaluate performance using EMA weights after each epoch

---

## Results

- Dataset: CIFAR-10
- Model: ViT-Small
- Training: From scratch
- Epochs: 200
- Test Accuracy: Approximately 88–90%

Results may vary slightly depending on hardware and random seed.

---

## References

- Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  https://arxiv.org/abs/2010.11929

- timm: PyTorch Image Models
  https://github.com/huggingface/pytorch-image-models

- CIFAR-10 Dataset
